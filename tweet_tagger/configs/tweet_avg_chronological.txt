# training
nepochs                 = 150
optimizer               = Adam
activation              = tanh
learning_rate           = 0.1
use_dropout             = True
ner_loss                = softmax#crf # or softmax

ner_classes             = BIO #or EC for entity classification
use_BIO_LSTM            = True
bin_features            = AVG # AVG, CNN, MAX, WORD_ATTENTION
bin_representation      = avg #max, avg
tweet_representation      = embeddings #LSTM, embeddings
non_linearity_bin_features = no
pad_length=30



batch_norm              = True
n_filters               = 20
filter_sizes            = [3, 4, 5]
dropout_cnn             = 0.1
cnn_pool                = max
dropout_lstm1_output    = 0

#hyperparameters
dropout_embedding       = 0.1
dropout_lstm2_output     = 0.1
dropout_fcl_ner         = 1
dropout_fcl_rel         = 1
hidden_dim              = 32
num_lstm_layers         = 1


# pretrained embeddings
pretrained_embeddings   = False
filename_embeddings     = "../../../twitter_data/glove_embeddings/glove.twitter.27B.25d.txt"

embeddings_size        = 25 # when pretrained =False

# dataset
filename_dev            = "/home/sai/Documents/MINI_PROJECT/SUB-EVENT-DETECTION/CODE/subevent_sequence_labeling-master/tweet_tagger/inputs/dev.csv"
filename_test           = "/home/sai/Documents/MINI_PROJECT/SUB-EVENT-DETECTION/CODE/subevent_sequence_labeling-master/tweet_tagger/inputs/test.csv"
filename_train          = "/home/sai/Documents/MINI_PROJECT/SUB-EVENT-DETECTION/CODE/subevent_sequence_labeling-master/tweet_tagger/inputs/train.csv"

